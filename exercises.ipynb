{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marctruter/Golden-Gate-Circuit-Calculator/blob/main/exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ePpH9H8Oekm"
      },
      "source": [
        "#  Computational Algebraic Geometry Research Network\n",
        "### Machine Learning and Algebraic Geometry, Practical tutorial\n",
        "Sara Veneziale, Imperial College London\n",
        "\n",
        "---\n",
        "\n",
        "This is a practical tutorial about machine learning and algebraic geometry. It has various small exercises about predicting the dimension of geometric objects from sequential data. Every time you see ```...``` it is something to fill in.\n",
        "\n",
        "The tutorial is structured as follows.\n",
        "\n",
        "- I: Jupyter notebooks\n",
        "- II: A recap of basic python syntax\n",
        "- III: Checking installation\n",
        "  \n",
        "The first part aims at predicting the dimension of a weighted projective space from some sequential data (the quantum period).\n",
        "- 1: First dataset download and data exploration\n",
        "- 2: Applying PCA\n",
        "- 3: Trying to predict the dimension\n",
        "- 4: Second dataset download and data exploration\n",
        "- 5: Applying PCA\n",
        "- 6: Trying to predict the dimension\n",
        "\n",
        "The second part aims at predicting the dimension of a polytope from some sequential data (its Ehrhart series). This part is less guided.\n",
        "- 7: Third dataset download and data exploration\n",
        "- 8: Applying PCA\n",
        "- 9: Predicting the dimension from the Log of the Ehrhart series\n",
        "- 10: Predicting the dimension from the Ehrhart series"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tLTEcmGzQItU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcag91iPOekq"
      },
      "source": [
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZamMKMR3Oekr"
      },
      "source": [
        "## I: Jupyter notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38XQd09nOekr"
      },
      "source": [
        "Jupyter notebooks integrate text and code. This is a markdown cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYK_Rd1aOekr"
      },
      "outputs": [],
      "source": [
        "# This is a code cell: execute it by pressing the triangle or pressing shift+enter\n",
        "m = 'This is a code cell!'\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO0xySlgOeks"
      },
      "outputs": [],
      "source": [
        "# Once a cell has been executed, the variable defined in it stay in memory (regardless of the order you have executed them)\n",
        "print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAm896DDOeks"
      },
      "source": [
        "# II: Introduction and Python syntax\n",
        "In this tutorial we will be using Python inside a Jupyter notebook. If you have never used Python before, we recap some of the basic syntax in the following cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW6sXy9bOeks"
      },
      "source": [
        "### Data types\n",
        "\n",
        "The basic data types in python (that we will encounter) are: integers, floats, strings, lists, dictionaries. Here are some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld-NhscaOekt"
      },
      "outputs": [],
      "source": [
        "# Integers\n",
        "an_integer = 1\n",
        "another_integer = 2\n",
        "\n",
        "# If you divide two integers, you do not get an integer, but a float (a 'real' number or a number with a '.')\n",
        "\n",
        "print('These are floats')\n",
        "print(1/2)\n",
        "print(2/1) # even if the division is exact\n",
        "\n",
        "# You can have anything packages by ' ' or \" \", and this will be a string\n",
        "a_string = 'hello'\n",
        "another_string = \"hello_again\"\n",
        "\n",
        "# You can package objects in a list\n",
        "an_empty_list = []\n",
        "a_list_of_integers = [1, 4, 5, 6]\n",
        "a_list_of_many_things = [1.0, 'hi!', 35]\n",
        "\n",
        "# Lists in Python are indexed from 0\n",
        "print('The first element of my list is')\n",
        "print(a_list_of_integers[0])\n",
        "\n",
        "# We can add elements to lists by appending\n",
        "print('My list has length')\n",
        "print(len(a_list_of_integers))\n",
        "\n",
        "a_list_of_integers.append(100)\n",
        "\n",
        "print('and now it has length')\n",
        "print(len(a_list_of_integers))\n",
        "\n",
        "# Dictionaries are like lists, where the indices can be (almost) anything\n",
        "an_empty_dictinary = {}\n",
        "oscar = {'Name': 'Oscar', 'Breed': 'Chihuahua', 'Age':5}\n",
        "\n",
        "print('How old is Oscar?')\n",
        "print(oscar['Age'])\n",
        "\n",
        "# We can add a key to my dictionary\n",
        "oscar['house'] = 'London'\n",
        "print('Where does Oscar live?')\n",
        "print(oscar['house'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S51Er-SsOekt"
      },
      "source": [
        "### Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOmtdYpbOekt"
      },
      "outputs": [],
      "source": [
        "# This is the structure of an if loop\n",
        "if 100 == a_list_of_integers[-1]: # == is equality, while = is assignment\n",
        "    print('I have appended 100 to the list')\n",
        "else:\n",
        "    print('I have note appended 100 to the list')\n",
        "\n",
        "# The following is a for loop\n",
        "print('Let us print the numbers from 1 to 4')\n",
        "for i in range(1,5): # range(1,5) = [1,2,3,4]\n",
        "    print(i)\n",
        "\n",
        "# The following is a list comprehension, it prints all even numbers between 0 and 20\n",
        "l = [2*i for i in range(1,10)]\n",
        "print('Let us print the even numbers greater than 0 and less than 20')\n",
        "print(l)\n",
        "\n",
        "# We can add conditions\n",
        "print('Let us print the even numbers greater than 0 and less than 20, not divisible by 5')\n",
        "print([x for x in l if x % 5 != 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KzuNuChOekt"
      },
      "source": [
        "## III: Checking installation\n",
        "Let us check if all the packages will import correctly. Execute the next cell, if it throws any errors please let me know!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyF9RQgEOekt"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import matplotlib\n",
        "import sklearn\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyGc7qE5Oeku"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: if you are running on Google Colab uncomment the following two lines and run this cell\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYR6_r9gOeku"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85OV8ZFLOeku"
      },
      "source": [
        "# 1: Data\n",
        "\n",
        "**Download the data of *regularised quantum periods* of weighted projective spaces** from [HERE](https://www.dropbox.com/scl/fi/sc98nnt0xpbbrxt2mj29m/periods1.txt?rlkey=wyd93zr0h001m82fbtxahdxmf&dl=0).\n",
        "\n",
        "The data looks like\n",
        "| Keys                   | Values                                                              |\n",
        "| ---------------------  | ------------------------------------------------------------------- |\n",
        "| $\\texttt{Weights}$     | A list of integers, e.g. $[1,1]$                                    |\n",
        "| $\\texttt{Dimension}$   | An integer                                                          |\n",
        "| $\\texttt{Periods}$     | A list of floats, e.g $[0.0, 0.69, 1.79, 2.99, ...]$                |\n",
        "\n",
        "It is not important for this tutorial to understand what these objects are! Here is what you need to know:\n",
        "- Weights are the weights of a weighted projective space (for example $[1,1]$ is $\\mathbb{P}^1$).\n",
        "- Dimension: an integer, the dimension of the weighted projective space.\n",
        "- Periods: a list of 2000 numbers. These are conjectured to be invariants of the weighted projective spaces.\n",
        "\n",
        "The following cell imports the data from 'periods1.txt' and saves it in a list 'data'. Each data sample is a dictionary with keys 'Weights', 'Dimension', 'Periods'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZK0zs4COeku"
      },
      "outputs": [],
      "source": [
        "# Uncomment the correct line and run this cell to load the data\n",
        "\n",
        "import ast\n",
        "\n",
        "data = []\n",
        "\n",
        "with open('periods1.txt', 'r') as f: # Leave this if you are running locally and your file in the same folder as the notebook\n",
        "# with open('/content/drive/MyDrive/periods1.txt', 'r') as f: # If you are on Colab uncomment this line and comment the line above\n",
        "    d = {}\n",
        "    for x in f:\n",
        "        if 'Weights' in x:\n",
        "            d['Weights'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'Dimension' in x:\n",
        "            d['Dimension'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'Periods' in x:\n",
        "            d['Periods'] = ast.literal_eval(x.split(': ')[1])\n",
        "            data.append(d)\n",
        "            d = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yisi8HFOeku"
      },
      "source": [
        "We want to understand if the 'Periods' know the 'Dimension' of each weighted projective space.\n",
        "\n",
        "Before we approach the question, let us look at basic things on our data. Data is a list containing dictionaries, each dictionary is a data sample at it has keys ```{'Weights', 'Dimension', 'Periods'} ```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xjUscm6Oeku"
      },
      "outputs": [],
      "source": [
        "# data is a list containing dictionaries\n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYr5IZ_ROeku"
      },
      "outputs": [],
      "source": [
        "# How many data samples we have? (Print the length of data)\n",
        "print(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZpOWxshOeku"
      },
      "outputs": [],
      "source": [
        "# Maximum dimension that appears\n",
        "dims_max = max([x['Dimension'] for x in data])\n",
        "\n",
        "# Minimum dimension that appears\n",
        "dims_min = min(...)\n",
        "\n",
        "# How many samples do we have for each dimension?\n",
        "for i in range(dims_min,dims_max+1): # Loop over all possible dimensions\n",
        "    print(f'Dimension {i}:')\n",
        "    print(len([x for x in data if ... ])) # print the length of the list of those samples that have dimension equal to i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y4rWESCOeku"
      },
      "outputs": [],
      "source": [
        "# Do all list of periods have the same length?\n",
        "len_of_list_of_periods = [len(x['Periods']) for x in data] # List containing the lengths of each period\n",
        "\n",
        "# Print the maximum and minimum length of periods\n",
        "print(max(...))\n",
        "print(min(...))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXKms7ovOeku"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-s67-ioOekv"
      },
      "source": [
        "## 2: Dimensionality Reduction\n",
        "\n",
        "Each period has 2000 terms, that is a lot of terms! We want to reduce the dimension of our data, by applying PCA (Principal Component Analysis). [Here is the sklearn link](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
        "\n",
        "In particular, let us apply PCA to the periods and look at the first two principal components.\n",
        "\n",
        "Here is a worked out example from the sklearn documentation:\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "# Random data\n",
        "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
        "\n",
        "# Define Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler and transform the features\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "# Define PCA with two components\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit and transform the data\n",
        "pca.fit(X)\n",
        "X = pca.transform(X)\n",
        "\n",
        "# Print the explained variance\n",
        "print(pca.explained_variance_ratio_)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQlMRDVWOekv"
      },
      "outputs": [],
      "source": [
        "# Isolate the periods data\n",
        "periods = [ ... for x in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWiijYU3Oekv"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Get the data and shuffle\n",
        "X = periods\n",
        "y = [ ... for x in data ] # the dimensions\n",
        "\n",
        "# Shuffle data\n",
        "X, y = shuffle(X, y, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SCGqk3HOekv"
      },
      "source": [
        "When we fit the PCA we will only do it to the training data, and transform both, so first we have to divide into training and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKB7vClKOekv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split training and testing (70% training and 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3 , shuffle = True, stratify = y)\n",
        "\n",
        "# This cell should fail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU-MqOPGOekv"
      },
      "source": [
        "You should get a mistake saying that the least populated class has only one number, which is too few. Let us exclude the dimension one and dimension two examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5qlD0IjOekv"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Get the data and shuffle\n",
        "X = [periods[i] for i in range(len(periods)) if data[i]['Dimension']>=3]\n",
        "y = [ ... for x in data if ... ] # the dimensions\n",
        "\n",
        "# Shuffle data\n",
        "X, y = shuffle(X, y, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG2Qh-PEOekv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split training and testing (70% training and 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= ... , shuffle = True, stratify = y)\n",
        "\n",
        "# This should now work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0faGckyAOekv"
      },
      "source": [
        "When applying PCA we need to standardise the data, this ensures that all features have equal importance in the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az9KgoH7Oekv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the scaler\n",
        "scaler = ...\n",
        "\n",
        "# Fit the scaler to the training data and transform the both training and testing\n",
        "scaler.fit( ... )\n",
        "X_train = scaler.transform( ... )\n",
        "X_test = scaler.transform( ... )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gulzKTqROekv"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define the PCA with two component\n",
        "pca = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV1djaupOekv"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the data\n",
        "pca.fit(...)\n",
        "X_train = pca.transform(...)\n",
        "X_test = pca.transform(...)\n",
        "\n",
        "# Print the explained variance\n",
        "print(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSkeTmZyOekw"
      },
      "source": [
        "The explained variance ratio is a measure of the proportion of the total variance in the original dataset that is explained by each principal component. You can think of it as 'how much information is carried in each component?' (with 0 being no information and 1 begin all the information).\n",
        "\n",
        "These components do not seem to carry a lot of information. Let us plot the two components against each other and color by dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S63x8KZ7Oekw"
      },
      "outputs": [],
      "source": [
        "# Nothing to do here, just execute the cell to plot the two PCA components against each other\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Apply the scaler to periods\n",
        "periods = scaler.transform(periods)\n",
        "\n",
        "# Apply the PCA to all periods\n",
        "periods = pca.transform(periods)\n",
        "\n",
        "# Plot the PCA results coloured by dimension\n",
        "x = [i[0] for i in periods]\n",
        "y = [i[1] for i in periods]\n",
        "c = [x['Dimension'] for x in data]\n",
        "\n",
        "# Classes for the legends\n",
        "classes = ['1','2','3','4','5','6','7','8','9','10']\n",
        "\n",
        "# Scatter plot\n",
        "scatter = plt.scatter(x, y, c=c, alpha=1, s=2)\n",
        "\n",
        "# Legend\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
        "\n",
        "# Labels\n",
        "plt.xlabel(r'PCA$_1$')\n",
        "plt.ylabel(r'PCA$_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_kbYS-FOekw"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ykEMdzOOekw"
      },
      "source": [
        "## 3: Predicting the dimension?\n",
        "\n",
        "Let us try to predict the dimension from the PCA component. We do not expect to do very well. Let us try to classify it using a Support Vector Machine, [ here is the sklearn documentation ](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) and [here is Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).\n",
        "\n",
        "Here is a worked out example from the documentation.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Random data\n",
        "X = np.array([[-1, -1], [1, 1], [-2, -1], [2, 1]]) # features\n",
        "y = np.array([1, 2, 1, 2]) # labels\n",
        "\n",
        "# Divide training and testing\n",
        "X_train = X[:2]\n",
        "X_test = X[2:]\n",
        "\n",
        "y_train = y[:2]\n",
        "y_test = y[2:]\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "scaler = StandardScaler() # define standard scaler\n",
        "svm = SVC(kernel = 'linear') # define svm (the kernel can be chosen from ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’)\n",
        "\n",
        "# Fit the scaler and transform the features\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Fit the SVM\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Compute prediction on the testing set\n",
        "predictions = svm.predict(X_test)\n",
        "\n",
        "# Compute the accuracy score\n",
        "print('Accuracy: ')\n",
        "print(accuracy_score(y_test, predictions))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiAJzinXOekz"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Define the svm with a kernel (you can try different ones!)\n",
        "svm = SVC(kernel = ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj9kqBaaOek0"
      },
      "outputs": [],
      "source": [
        "# Fit the SVM to the training data\n",
        "svm.fit( ... , ... )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWurHTTlOek0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Compute prediction on the testing set\n",
        "predictions = svm.predict( ... )\n",
        "\n",
        "# Compute the accuracy score between y_test and predictions\n",
        "print('Accuracy: ')\n",
        "print(accuracy_score( ... , ... ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGE76re5Oek0"
      },
      "source": [
        "Why is this doing so badly? Let us look at the data more closely. Print the number of zeros that appear in each periods and plot a histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz4AACx7Oek0"
      },
      "outputs": [],
      "source": [
        "# Find the number of zeros for each periods\n",
        "zeros = [len([y for y in x['Periods'] if ... ]) for x in data]\n",
        "\n",
        "_ = plt.hist(zeros, bins = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiBhXwEbOek0"
      },
      "source": [
        "Most of the data is zero!!! This might be the reason why our classification problem was not very good. Let us download more data, where we have computed more coefficients ofthe periods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rwtML7vOek0"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IrhgTpJOek0"
      },
      "source": [
        "# 4: Data\n",
        "\n",
        "**Download the data of *regularised quantum periods* of weighted projective spaces** from [HERE](https://www.dropbox.com/scl/fi/qr3hs25h5mfea9krmjbyk/periods_more.txt?rlkey=v81hueis97kn3nkrg775xfjw5&dl=0). In this case we compute a lot more terms, and record only those that are non-zero.\n",
        "\n",
        "The data looks like\n",
        "| Keys                   | Values                                                              |\n",
        "| ---------------------  | ------------------------------------------------------------------- |\n",
        "| $\\texttt{Weights}$     | A list of integers, e.g. $[1,1]$                                    |\n",
        "| $\\texttt{Dimension}$   | An integer                                                          |\n",
        "| $\\texttt{Indices}$     | A list of integers, e.g. $[0, 2, 4, 6, 8, 10, 12, 14, ... ]$        |\n",
        "| $\\texttt{Periods}$     | A list of floats, e.g $[0.0, 0.69, 1.79, 2.99, ...]$                |\n",
        "\n",
        "It is not important for this tutorial to understand what these objects are! Here is what you need to know:\n",
        "- Weights are the weights of a weighted projective space (for example $[1,1]$ is $\\mathbb{P}^1$).\n",
        "- Dimension: an integer, the dimension of the weighted projective space.\n",
        "- Indices: a list of integers, the indices of the non-zero terms of the periods.\n",
        "- Periods: a list of floats, only the non-zero terms of the periods.\n",
        "Note in this case we only record those elements of periods that are non-zero, and their indices are recorded in Indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAVRReODOek0"
      },
      "outputs": [],
      "source": [
        "# Uncomment the correct line and run this cell to load the data\n",
        "\n",
        "import ast\n",
        "\n",
        "data = []\n",
        "\n",
        "with open('periods_more.txt', 'r') as f: # Leave this if you are running locally and your file in the same folder as the notebook\n",
        "# with open('/content/drive/MyDrive/periods_more.txt', 'r') as f: # If you are on Colab uncomment this line and comment the line above\n",
        "    d = {}\n",
        "    for x in f:\n",
        "        if 'Weights' in x:\n",
        "            d['Weights'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'Dimension' in x:\n",
        "            d['Dimension'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'Indices' in x:\n",
        "            d['Indices'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'Periods' in x:\n",
        "            d['Periods'] = ast.literal_eval(x.split(': ')[1])\n",
        "            data.append(d)\n",
        "            d = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4uc5UJ_Oek0"
      },
      "outputs": [],
      "source": [
        "# How many data samples we have?\n",
        "print( ... )\n",
        "\n",
        "# What is the max dimension and the min dimension?\n",
        "dims_max = ...\n",
        "dims_min = ...\n",
        "\n",
        "# How many samples do we have for each dimension?\n",
        "for i in range( ... , ... ):\n",
        "    print(f'Dimension {i}:')\n",
        "    print(len( ... ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FvB-dSFOek0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLBSzaZkOek1"
      },
      "source": [
        "## 5: Dimensionality reduction using PCA\n",
        "\n",
        "Let us try to predict the dimension from the PCA component, apply PCA with two components to the periods data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu9sZqLwOek1"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Isolate the periods data\n",
        "periods = [ ... for x in data]\n",
        "\n",
        "# Ignore dim 1 and 2 because there are too few examples\n",
        "X = [periods[i] for i in range(len(periods)) if  ... ]\n",
        "y = [x['Dimension'] for x in data if ... ]\n",
        "\n",
        "# Shuffle data\n",
        "X, y = shuffle( ... )\n",
        "\n",
        "# Train test split: 70% for training and 30% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(... , ... , test_size= ... , shuffle = True, stratify = y)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define scaler\n",
        "scaler = ...\n",
        "\n",
        "# Scale the features\n",
        "scaler.fit(...)\n",
        "X_train = scaler.transform(...)\n",
        "X_test = scaler.transform(...)\n",
        "\n",
        "# Define the PCA with two components\n",
        "pca = ...\n",
        "\n",
        "# Fit and transform the data\n",
        "pca.fit(...)\n",
        "X_train = pca.transform(...)\n",
        "X_test = pca.transform(...)\n",
        "\n",
        "# Print the explained variance\n",
        "print( ... )\n",
        "\n",
        "# This cell should fail!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CsUt0KSOek1"
      },
      "source": [
        "You should have got a ValueError! This is because our periods are not the same length anymore, and PCA only works if our input all have the same dimension. Compute the maximum and minimum length of periods that appear in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a41YEs2pOek1"
      },
      "outputs": [],
      "source": [
        "# Do all list of periods have the same length?\n",
        "len_of_list_of_periods = [len( ... ) for x in data]\n",
        "\n",
        "# Print the maximum lenght and the minimum lenght that appears\n",
        "print(max( ... ))\n",
        "print(min( ... ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZNAl6ZOek1"
      },
      "source": [
        "Let us chop all the periods so that they have the same length as the vector with minimum length appearing in periods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qlcqIq5Oek1"
      },
      "outputs": [],
      "source": [
        "# Define minimum\n",
        "min_len_periods = min( ... )\n",
        "\n",
        "# Truncate all the log periods so that they have length m\n",
        "for x in data:\n",
        "    x['Periods'] = x['Periods'][:min_len_periods]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcI8qzl3Oek1"
      },
      "outputs": [],
      "source": [
        "# Now check that the max and the min of the lengths of the periods are the same\n",
        "len_of_list_of_periods = [len( ... ) for x in data]\n",
        "\n",
        "# Print the maximum lenght and the minimum lenght that appears\n",
        "print(max( ... ))\n",
        "print(min( ... ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lul64JYtOek1"
      },
      "source": [
        "Now that all periods have the same length we can apply PCA with two components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pCGe5d5OelJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Isolate the periods data\n",
        "periods = ...\n",
        "\n",
        "# Ignore dim 1 and 2 because there are too few examples\n",
        "X = [periods[i] for i in range(len(periods)) if  ... ]\n",
        "y = [x['Dimension'] for x in data if ... ]\n",
        "\n",
        "# Shuffle data\n",
        "X, y = ...\n",
        "\n",
        "# Train test split: 70% for training and 30% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(... , ... , test_size= ... , shuffle = True, stratify = y)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define scaler\n",
        "scaler = ...\n",
        "\n",
        "# Scale the features\n",
        "scaler.fit(...)\n",
        "X_train = scaler.transform(...)\n",
        "X_test = scaler.transform(...)\n",
        "\n",
        "# Define the PCA with two components\n",
        "pca = ...\n",
        "\n",
        "# Fit and transform the data\n",
        "pca.fit(...)\n",
        "X_train = pca.transform(...)\n",
        "X_test = pca.transform(...)\n",
        "\n",
        "# Print the explained variance\n",
        "print( ... )\n",
        "\n",
        "# This should now work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMwiFF1jOelK"
      },
      "source": [
        "Let us plot the two components against each other and color by dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1nnrDmxOelK"
      },
      "outputs": [],
      "source": [
        "# Nothing to do here, just execute the cell to plot the two PCA components against each other\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "periods = scaler.transform(periods)\n",
        "periods = pca.transform(periods)\n",
        "\n",
        "# Plot the PCA results coloured by dimension\n",
        "x = [i[0] for i in periods]\n",
        "y = [i[1] for i in periods]\n",
        "c = [d['Dimension'] for d in data]\n",
        "\n",
        "# Classes for the legends\n",
        "classes = ['1','2','3','4','5','6','7','8','9','10']\n",
        "\n",
        "# Scatter plot\n",
        "scatter = plt.scatter(x, y, c=c, alpha=1, s=2)\n",
        "\n",
        "# Legend\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
        "\n",
        "# Labels\n",
        "plt.xlabel(r'PCA$_1$')\n",
        "plt.ylabel(r'PCA$_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDUkcghbOelK"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj8sXgNgOelL"
      },
      "source": [
        "## 6: Predicting the dimension?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygnh5C2wOelL"
      },
      "source": [
        "The PCA components are clearly separable by lines into the different dimensions clusters. Let us fit an SVM with linear kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAJcN6lwOelL"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the svm\n",
        "clf = svm.SVC(...)\n",
        "\n",
        "# Train the svm\n",
        "clf.fit(..., ...)\n",
        "\n",
        "# Compute prediction on the testing set\n",
        "predictions = clf.predict(...)\n",
        "\n",
        "# Compute the accuracy score\n",
        "print('Accuracy: ')\n",
        "print(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE8F5iP5OelL"
      },
      "source": [
        "This is not as good of an accuracy as we would have expected. Try and standardise the data again and see if it helps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG_HpiVwOelL"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define scaler\n",
        "scaler2 = ...\n",
        "\n",
        "# Scale the features\n",
        "scaler2.fit(...)\n",
        "X_train = scaler2.transform(...)\n",
        "X_test = scaler2.transform(...)\n",
        "\n",
        "# Define the svm\n",
        "clf = svm.SVC(...)\n",
        "\n",
        "# Train the svm\n",
        "clf.fit(..., ...)\n",
        "\n",
        "# Compute prediction on the testing set\n",
        "predictions = clf.predict(...)\n",
        "\n",
        "# Compute the accuracy score\n",
        "print('Accuracy: ')\n",
        "print(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsTQ-K3-OelL"
      },
      "source": [
        "Plot the learning boundaries computed by the SVM on top of a scatter plot of the scaled PCA components. Note that the SVM should compute 28 decision boundaries, only plot those with indices [0, 7, 13, 18, 22, 25, 27], which correspond to decision boundaries for neighbour dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mixelRB6OelM"
      },
      "outputs": [],
      "source": [
        "# Nothing to do here, just execute the cell to see the plot\n",
        "import numpy as np\n",
        "plt.clf()\n",
        "\n",
        "# Isolate the periods data\n",
        "periods = [ x['Periods'] for x in data]\n",
        "\n",
        "# Scale all features\n",
        "periods = scaler.transform(periods)\n",
        "periods = pca.transform(periods)\n",
        "periods = scaler2.transform(periods)\n",
        "\n",
        "y = [x['Dimension'] for x in data]\n",
        "\n",
        "# Coefficients and y-intercepts of the decision boundaries from the trained svm\n",
        "coeffs = clf.coef_\n",
        "intercepts = clf.intercept_\n",
        "\n",
        "# Extract PCA components\n",
        "scaled_PCA1 = [x[0] for x in periods]\n",
        "scaled_PCA2 = [x[1] for x in periods]\n",
        "\n",
        "# Plot the decision boundaries between neighbouring classes\n",
        "for i in [0, 7, 13, 18, 22, 25, 27]:\n",
        "    x = np.linspace(-2, 4, 1000)\n",
        "    f = (-coeffs[i][0]*x-intercepts[i])/coeffs[i][1]\n",
        "    plt.plot(x, f, color = 'black', linewidth = 0.5)\n",
        "\n",
        "# Scatter plot\n",
        "ax = plt.scatter(scaled_PCA1, scaled_PCA2, c = y, alpha = 1, s = 2)\n",
        "\n",
        "# x-label\n",
        "plt.xlabel(r'standardised PCA$_1$')\n",
        "\n",
        "# y-label\n",
        "plt.ylabel(r'standardised PCA$_2$')\n",
        "\n",
        "# Legend\n",
        "plt.legend(handles = ax.legend_elements()[0], labels=classes[2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWimwZIIOelM"
      },
      "source": [
        "--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fobxWi9SOelM"
      },
      "source": [
        "## 7: Data, the dimension of a polytope\n",
        "\n",
        "This example is taken from `Machine learning the dimension of a polytope', T. Coates, J. Hofscheirer, A. M. Kasprzyk.\n",
        "\n",
        "The aim of this part is to predict the dimension of a polytope from its Ehrhart series.\n",
        "\n",
        "**Download the data of Ehrhart series and dimensions** from [HERE](https://zenodo.org/records/6614821) and save it to the same folder as this notebook.\n",
        "\n",
        "The data looks like\n",
        "| Keys                   | Values                                                              |\n",
        "| ---------------------  | ------------------------------------------------------------------- |\n",
        "| $\\texttt{ULID}$        | A string                                                            |\n",
        "| $\\texttt{Dimension}$   | An integer                                                          |\n",
        "| $\\texttt{Volume}$      | An integer                                                          |\n",
        "| $\\texttt{EhrhartDelta}$| A list of integers, e.g. $[1,70,223,48]$                            |\n",
        "| $\\texttt{Ehrhart}$     | A list of integers, e.g. $[1,74,513,...]$                           |\n",
        "| $\\texttt{LogEhrhart}$  | A list of floats, e.g $[0.0,4.3,6.2,...]$                           |\n",
        "\n",
        "It is not important for this tutorial to understand what these objects are! Here is what you need to know:\n",
        "- ULID is just to keep track of which example we are looking at.\n",
        "- Dimension: an integer, the dimension of a polytope $d$\n",
        "- Volume: an integer, the volume of a polytope\n",
        "- EhrhartDelta: a sequence of integers of length $d+1$\n",
        "- Ehrhart: a sequence of integers\n",
        "- LogEhrhart: a sequence of floats (the log of the previous one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKmMho7DOelM"
      },
      "outputs": [],
      "source": [
        "# Uncomment the correct line and run this cell to load the data\n",
        "\n",
        "import ast\n",
        "\n",
        "data = []\n",
        "\n",
        "with open('dimension.txt', 'r') as f: # Leave this if you are running locally and your file in the same folder as the notebook\n",
        "# with open('/content/drive/MyDrive/dimension.txt', 'r') as f: # If you are on Colab uncomment this line and comment the line above\n",
        "    d = {}\n",
        "    for x in f:\n",
        "        if 'Dimension' in x:\n",
        "            d['Dimension'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'Volume' in x:\n",
        "            d['Volume'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'EhrhartDelta' in x:\n",
        "            d['EhrhartDelta'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'Ehrhart' in x and 'Log' not in x and 'Delta' not in x:\n",
        "            d['Ehrhart'] = ast.literal_eval(x.split(': ')[1])\n",
        "        elif 'LogEhrhart' in x:\n",
        "            d['LogEhrhart'] = ast.literal_eval(x.split(': ')[1])\n",
        "            data.append(d)\n",
        "            d = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BSmCNNvOelM"
      },
      "source": [
        "As before, print how many samples we have and how many samples for each dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA6M33KmOelM"
      },
      "outputs": [],
      "source": [
        "# How many examples do we have\n",
        "print(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-PTcrrXOelN"
      },
      "outputs": [],
      "source": [
        "# How many samples do we have for each dimension?\n",
        "dims_max = ...\n",
        "dims_min = ...\n",
        "\n",
        "\n",
        "for i in ...:\n",
        "    print(f'Dimension {i}:')\n",
        "    print(...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX3an-IfOelN"
      },
      "outputs": [],
      "source": [
        "# Do all list of series have the same length?\n",
        "len_of_log_ehrhart = ...\n",
        "\n",
        "# Print the maximum lenght and the minimum lenght that appears\n",
        "print(max(...))\n",
        "print(min(...))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_G5a3IBOelN"
      },
      "outputs": [],
      "source": [
        "# Do all list of series have the same length?\n",
        "len_of_ehrhart = ...\n",
        "\n",
        "# Print the maximum lenght and the minimum lenght that appears\n",
        "print(max(...))\n",
        "print(min(...))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMb3jqAyOelN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPSnN5VzOelN"
      },
      "source": [
        "## 8: Dimensionality reduction of the logarithm of the Ehrhat vector\n",
        "\n",
        "Apply PCA with two components to the logarithmic Ehrhat vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "netYYu-tOelN"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Periods data\n",
        "logehrhart = ...\n",
        "\n",
        "# Isolate features and labels\n",
        "X = ...\n",
        "y = ...\n",
        "\n",
        "# Shuffle data\n",
        "X, y = ...\n",
        "\n",
        "# Train test split: 70% for training and 30% testing\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "# Define scaler\n",
        "...\n",
        "\n",
        "# Fit the scaler\n",
        "...\n",
        "\n",
        "# Scale both the training and the testing data\n",
        "...\n",
        "...\n",
        "\n",
        "# Define the PCA with two components\n",
        "...\n",
        "\n",
        "# Fit the data and transform\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "# Print the explained variance\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSpjueYhOelN"
      },
      "source": [
        "Plot the two principal components against each other and colour by dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQrKw9RsOelO"
      },
      "outputs": [],
      "source": [
        "# Nothing to do here, just execute the cell for plotting\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "logehrhart = scaler.transform(logehrhart)\n",
        "logehrhart = pca.transform(logehrhart)\n",
        "\n",
        "# Plot the PCA results coloured by dimension\n",
        "x = [i[0] for i in logehrhart]\n",
        "y = [i[1] for i in logehrhart]\n",
        "c = [x['Dimension'] for x in data]\n",
        "\n",
        "# Classes for the legends\n",
        "classes = ['2','3','4','5','6','7','8']\n",
        "\n",
        "# Scatter plot\n",
        "scatter = plt.scatter(x, y, c=c, alpha=1, s=2)\n",
        "\n",
        "# Legend\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
        "\n",
        "# Labels\n",
        "plt.xlabel(r'PCA$_1$')\n",
        "plt.ylabel(r'PCA$_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTpBl8nSOelO"
      },
      "source": [
        "These are clearly linearly separable. Train a linear Support Vector Machine to predict the dimension from the two components of the PCA and plot the linear boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnfkYpViOelO"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Define another scaler\n",
        "...\n",
        "\n",
        "# Fit the scaler\n",
        "...\n",
        "\n",
        "# Scale both the training and the testing data\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "# Define the svm\n",
        "...\n",
        "\n",
        "# Train the svm\n",
        "...\n",
        "\n",
        "# Compute prediction on the testing set\n",
        "...\n",
        "\n",
        "# Compute the accuracy score\n",
        "print('Accuracy: ')\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUEIxmzbOelO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL4gYYNkOelO"
      },
      "source": [
        "## 10: Predicting the dimension from the Ehrhart vetor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sijyB1aOelO"
      },
      "source": [
        "Try to do the same using the Ehrhart vector, instead of LogEhrhart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hGtcjB4OelO"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Periods data\n",
        "ehrhart = ...\n",
        "\n",
        "# Isolate features and labels\n",
        "X = ehrhart\n",
        "y = [x['Dimension'] for x in data]\n",
        "\n",
        "# Shuffle data\n",
        "...\n",
        "\n",
        "# Train test split: 70% for testing\n",
        "...\n",
        "\n",
        "# Define the PCA with two components\n",
        "...\n",
        "\n",
        "# Define scaler and scale the features\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "# Fit the data and transform the data\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "# Print the explained variance\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chB3DDSKOelP"
      },
      "outputs": [],
      "source": [
        "# Nothing to do here, just execute the cell to plot the picture\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "ehrhart = scaler.transform(ehrhart)\n",
        "ehrhart = pca.transform(ehrhart)\n",
        "\n",
        "# Plot the PCA results coloured by dims\n",
        "x = [i[0] for i in ehrhart]\n",
        "y = [i[1] for i in ehrhart]\n",
        "c = [x['Dimension'] for x in data]\n",
        "\n",
        "# Classes for the legends\n",
        "classes = ['2','3','4','5','6','7','8']\n",
        "\n",
        "# Scatter plot\n",
        "scatter = plt.scatter(x, y, c=c, alpha=1, s=2)\n",
        "\n",
        "# Legend\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
        "\n",
        "# Labels\n",
        "plt.xlabel(r'PCA$_1$')\n",
        "plt.ylabel(r'PCA$_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tewnFDdnOelP"
      },
      "source": [
        "Try to predict the dimension from the first two PCA components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8btKMZGKOelP"
      },
      "outputs": [],
      "source": [
        "# Define scaler and scale the features\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "# Define the svm, fit it, and compute the predictions for the test set\n",
        "...\n",
        "\n",
        "# Compute the accuracy score\n",
        "print('Accuracy: ')\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Lz2fk3OelP"
      },
      "source": [
        "Maybe in this case, two components are not enough. Write a function that takes in as arguments\n",
        "- the features (the ehrhart vector)\n",
        "- the labels (the dimensions)\n",
        "- the numbers of components of the PCA\n",
        "- the percentage of testing data\n",
        "\n",
        "And the returns the accuracy obtained by a linear SVM trained on the data after it has gone through PCA with the specified number of components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZWsjGCdOelP"
      },
      "outputs": [],
      "source": [
        "def Accuracy(features, labels, n_components, testing):\n",
        "    ''' Returns the accuracy of a linear SVM.'''\n",
        "...\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy1D8JO-OelP"
      },
      "outputs": [],
      "source": [
        "# Periods data\n",
        "ehrhart = [x['Ehrhart'] for x in data]\n",
        "y = [x['Dimension'] for x in data]\n",
        "\n",
        "n_components = 2 # Play around with the number of components and see how the accuracy changes\n",
        "print(Accuracy(ehrhart, y, n_components, 0.3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}